{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8ec645e",
   "metadata": {},
   "source": [
    "### TASKS 1. Implement Q-learning algorithm using OpenAI gym environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dccf9ba1",
   "metadata": {},
   "source": [
    "## TASK-1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9aed0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import gym\n",
    "\n",
    "\n",
    "def q_learning(env, num_episodes=10000, learning_rate=0.8, discount_factor=0.95, exploration_prob=0.2):\n",
    " \n",
    "    q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "         \n",
    "            if random.uniform(0, 1) < exploration_prob:\n",
    "                action = env.action_space.sample()  \n",
    "            else:\n",
    "                action = np.argmax(q_table[state, :])  \n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "            \n",
    "            q_table[state, action] = q_table[state, action] + learning_rate * (\n",
    "                    reward + discount_factor * np.max(q_table[next_state, :]) - q_table[state, action])\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "    return q_table\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make('FrozenLake-v1')\n",
    "    num_episodes = 10000\n",
    "    q_table = q_learning(env, num_episodes=num_episodes)\n",
    "\n",
    "    num_test_episodes = 100\n",
    "    num_successes = 0\n",
    "\n",
    "    for i in range(num_test_episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action = np.argmax(q_table[state, :])\n",
    "            next_state, _, done, _ = env.step(action)\n",
    "            state = next_state\n",
    "\n",
    "            if done and state == 15:  \n",
    "                num_successes += 1\n",
    "\n",
    "    success_rate = num_successes / num_test_episodes\n",
    "    print(f\"Success rate over {num_test_episodes} episodes: {success_rate}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966b7948",
   "metadata": {},
   "source": [
    "#####  TASK-2  The Smartcab&#39;s job is to pick up the passenger at one location and drop them off in another. The agent should receive a high positive reward for a successful drop-off because this behavior is highly desired The agent should be penalized if it tries to drop off a passenger in wrong locations The agent should get a slight negative reward for not making it to the destination after every time-step. The passenger can be in one of the four possible locations: R, G, Y, B, which are represented in row, column coordinates as (0,0), (0,4), (4,0), (4,3) respectively. Additionally, we need to consider a fifth state where the passenger is already inside the taxi. Therefore, the number of possible states for the passenger&#39;s location is 5. The destination can be one of the four possible locations: R, G, Y, B, which are also represented in row, column coordinates. Therefore, the number of possible states for the destination is 4. We have six possible actions:\n",
    "1. south\n",
    "2. north\n",
    "3. east\n",
    "4. west\n",
    "5. pickup\n",
    "6. dropoff\n",
    "#### Implement the above problem using Gym environment called Taxi-V2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc238c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make('Taxi-v3')\n",
    "\n",
    "\n",
    "def get_reward(state, next_state, done):\n",
    "    if done and next_state == 4:  \n",
    "        return 100\n",
    "    elif done and next_state != 4:  \n",
    "        return -10\n",
    "    elif state == next_state:  \n",
    "        return -1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def q_learning(env, num_episodes=10000, learning_rate=0.8, discount_factor=0.95, exploration_prob=0.2):\n",
    "    \n",
    "    q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            \n",
    "            if random.uniform(0, 1) < exploration_prob:\n",
    "                action = env.action_space.sample()  \n",
    "            else:\n",
    "                action = np.argmax(q_table[state, :])  \n",
    "\n",
    "            next_state, _, done, _ = env.step(action)\n",
    "\n",
    "        \n",
    "            reward = get_reward(state, next_state, done)\n",
    "\n",
    "            \n",
    "            q_table[state, action] = q_table[state, action] + learning_rate * (\n",
    "                    reward + discount_factor * np.max(q_table[next_state, :]) - q_table[state, action])\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "    return q_table\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import numpy as np\n",
    "    import random\n",
    "\n",
    "    num_episodes = 10000\n",
    "    q_table = q_learning(env, num_episodes=num_episodes)\n",
    "\n",
    "    \n",
    "    num_test_episodes = 100\n",
    "    num_successes = 0\n",
    "\n",
    "    for i in range(num_test_episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action = np.argmax(q_table[state, :])\n",
    "            next_state, _, done, _ = env.step(action)\n",
    "            state = next_state\n",
    "\n",
    "            if done and state == 4:  \n",
    "                num_successes += 1\n",
    "\n",
    "    success_rate = num_successes / num_test_episodes\n",
    "    print(f\"Success rate over {num_test_episodes} episodes: {success_rate}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02001fa7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
